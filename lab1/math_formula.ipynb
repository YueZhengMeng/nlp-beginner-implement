{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 线性层公式\n",
    "\n",
    "## 1.1 线性性层的前向传播\n",
    "$$\n",
    "\\boldsymbol{Y}=\\boldsymbol{XW}+\\left( \\boldsymbol{1}_{p\\times 1} \\right) \\boldsymbol{b} \\tag{1.1}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$X$是输入 : (batch_size, input_size)\n",
    "$W$是权重 : (input_size, output_size)\n",
    "$b$是偏置 : (1, output_size)\n",
    "$Y$是输出 : (batch_size, output_size)\n",
    "$p$是batch_size, $(\\boldsymbol{1}_{p\\times 1})$表示$b$会在batch_size上进行广播"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86ee836ec512020b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 线性性层的反向传播\n",
    "### 1.2.1 对权重矩阵$W$的梯度\n",
    "$$ \n",
    "\\nabla _{\\boldsymbol{W}}L=\\left( \\boldsymbol{X}^{\\top} \\right) \\left( \\nabla _{\\boldsymbol{Y}}L \\right) \\tag{1.2}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$X^T$是输入的转置 : (input_size, batch_size)\n",
    "$\\nabla _{\\boldsymbol{Y}}L$是损失函数对本层输出的梯度 : (batch_size, output_size)\n",
    "$\\nabla _{\\boldsymbol{W}}L$是损失函数对本层权重的梯度 : (input_size, output_size)\n",
    "\n",
    "### 1.2.2 对偏置$b$的梯度\n",
    "$$\n",
    "\\nabla _{\\boldsymbol{b}}L=\\left( \\boldsymbol{1}_{1\\times p} \\right) \\left( \\nabla _{\\boldsymbol{Y}}L \\right) \\tag{1.3}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$\\nabla _{\\boldsymbol{Y}}L$是损失函数对本层输出的梯度 : (batch_size, output_size)\n",
    "$p$是batch_size, $(\\boldsymbol{1}_{1\\times p})$表示$\\nabla _{\\boldsymbol{b}}L$会在batch_size上进行反向广播，即求和\n",
    "$\\nabla _{\\boldsymbol{b}}L$是损失函数对本层偏置的梯度 : (1, output_size)\n",
    "\n",
    "### 1.2.3 对输入$X$的梯度\n",
    "$$\n",
    "\\nabla _{\\boldsymbol{X}}L=\\left( \\nabla _{\\boldsymbol{Y}}L \\right)\\left( \\boldsymbol{W}^{\\top} \\right) \\tag{1.4}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$\\nabla _{\\boldsymbol{Y}}L$是损失函数对本层输出的梯度 : (batch_size, output_size)\n",
    "$W^T$是权重的转置 : (output_size, input_size)\n",
    "$\\nabla _{\\boldsymbol{X}}L$是损失函数对本层输入的梯度 : (batch_size, input_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80768df658b88eba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 线性性层的代码实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dbc79d2830b4a34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # 初始化权重矩阵和偏置向量\n",
    "        self.weights = np.random.normal(loc=0.0, scale=0.1, size=(input_size, output_size))\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "\n",
    "        # 存储输入输出用于反向传播\n",
    "        self.x = None\n",
    "\n",
    "        # 存储梯度用于反向传播\n",
    "        self.d_weights = None\n",
    "        self.d_bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 备份输入x，反向传播时会用到\n",
    "        self.x = x\n",
    "        # 前向传播计算输出，对应公式(1.1)\n",
    "        # bias会自动broadcasting到(batch_size, output_size)维\n",
    "        output = np.matmul(self.x, self.weights) + self.bias\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # 反向传播计算权重的梯度，对应公式(1.2)\n",
    "        self.d_weights = np.matmul(self.x.T, d_output)\n",
    "        # 反向传播计算偏置的梯度，对应公式(1.3)\n",
    "        self.d_bias = np.sum(d_output, axis=0)\n",
    "        # 反向传播计算输入的梯度，对应公式(1.4)\n",
    "        d_input = np.matmul(d_output, self.weights.T)\n",
    "        return d_input\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # 使用梯度下降法更新权重和偏置\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.bias -= learning_rate * self.d_bias"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T12:29:15.784079600Z",
     "start_time": "2024-06-16T12:29:15.764421Z"
    }
   },
   "id": "e2e30a9ac3f15e41",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 ReLU激活函数层公式\n",
    "# 2.1 ReLU激活函数层的前向传播\n",
    "$$\n",
    "\\boldsymbol{Y}=\\max \\left( \\boldsymbol{X}, 0 \\right) \\tag{2.1}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$X$是输入 : (batch_size, input_size)\n",
    "$Y$是输出 : (batch_size, input_size)\n",
    "一句话描述：将$X$中小于0的值置为0，大于等于0的值保持不变"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b9fc3a068a42a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 ReLU激活函数层的反向传播\n",
    "### 2.2.1 对输入$X$的梯度\n",
    "$$\n",
    "\\nabla _{\\boldsymbol{X}}L=\\begin{cases} \t0,&x<0\\\\ \t\\nabla _{\\boldsymbol{Y}}L,&x\\geqslant 0\\\\ \\end{cases} \\tag{2.2}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$\\nabla _{\\boldsymbol{Y}}L$是损失函数对本层输出的梯度 : (batch_size, input_size)\n",
    "$\\nabla _{\\boldsymbol{X}}L$是损失函数对本层输入的梯度 : (batch_size, input_size)\n",
    "一句话描述：$X$中小于0的值对应的梯度为0，大于等于0的值对应的梯度为$\\nabla _{\\boldsymbol{Y}}L$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc80fe16edf6e83e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 ReLU激活函数层的代码实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a825719bb26019cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 备份输入x，反向传播时会用到\n",
    "        self.x = x\n",
    "        # 前向传播计算，对应公式(2.1)\n",
    "        output = np.maximum(0, x)\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # 反向传播的计算，对应公式(2.2)\n",
    "        d_input = d_output\n",
    "        d_input[self.x < 0] = 0\n",
    "        return d_input"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T12:29:15.828133800Z",
     "start_time": "2024-06-16T12:29:15.790107500Z"
    }
   },
   "id": "75d221dd567127b2",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Softmax函数公式\n",
    "## 3.1 Softmax函数的前向计算\n",
    "$$\n",
    "\\boldsymbol{\\hat{Y}}_{i,j}  =\\frac{\\exp \\left[ \\boldsymbol{X}_{i,j} \\right]}{\\displaystyle \\sum_j{\\exp \\left[ \\boldsymbol{X}_{i,j} \\right]}} \\tag{3.1}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$X$是已经通过线性层从(batch_size, hidden_size)映射到(batch_size, num_class)的输入\n",
    "$\\hat{Y}$是输出 : (batch_size, num_class), $\\hat{Y}_{i,j}$表示第$i$个样本属于第$j$类的概率"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3f7a58ffd3a6e02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "考虑到${X}_{i,j}$较大时，$\\exp \\left[ \\boldsymbol{X}_{i,j} \\right]$可能会溢出，所以我们可以对公式(3.1)稍作变形：\n",
    "$$\n",
    "\\boldsymbol{\\hat{Y}}_{i,j}=\\frac{\\displaystyle \\exp \\left[ \\boldsymbol{X}_{i,j}-\\max_n \\boldsymbol{X}_{i,n} \\right]}{\\displaystyle \\sum_j{\\exp \\left[ \\boldsymbol{X}_{i,j}-\\max_n \\boldsymbol{X}_{i,n} \\right]}} \\tag{3.2}\n",
    "$$\n",
    "对于每一个样本，使其特征向量中每一项都减去最大项$\\max_n \\boldsymbol{X}_{i,n}$不会改变结果，但是可以避免溢出"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9273f9b54bf706bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Softmax函数的反向传播\n",
    "### 3.2.1 对输入$X$的梯度\n",
    "$$\n",
    "\\frac{\\partial \\boldsymbol{\\hat{y}}_{i,k}}{\\partial \\boldsymbol{x}_{i,j}}=\\frac{\\partial}{\\partial \\boldsymbol{x}_{i,j}}\\frac{\\exp \\left[ \\boldsymbol{X}_{i,k} \\right]}{\\sum_k{\\exp \\left[ \\boldsymbol{X}_{i,k} \\right]}}=\\begin{cases} \t-\\boldsymbol{\\hat{y}}_{i,j}\\boldsymbol{\\hat{y}}_{i,k},&k\\ne j\\\\ \t-\\boldsymbol{\\hat{y}}_{i,j}\\boldsymbol{\\hat{y}}_{i,k}+\\boldsymbol{\\hat{y}}_{i,j},&k=j\\\\ \\end{cases} \\tag{3.3}\n",
    "$$\n",
    "其中：\n",
    "$\\boldsymbol{\\hat{y}}_{i,k}$表示样本$i$属于类别$k$的概率，其在对特征向量$\\boldsymbol{x}_{i}$中的每一项求偏导数时，根据softmax函数的公式与函数除法的求导法则，可以得知，在${x}_{i,j}$项只存在于分母时和${x}_{i,j}$项同时存在于分子和分母时，对应的偏导数不同"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aafdf1f74476fff4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Softmax函数的代码实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8db19a6e0799e3f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def softmax(input):\n",
    "    # 通过softmax函数计算概率\n",
    "    # 减去输入的最大值，防止指数爆炸\n",
    "    # 对应公式(3.2)\n",
    "    input_max = np.max(input, axis=1, keepdims=True)\n",
    "    input_exp = np.exp(input - input_max)\n",
    "    # 计算概率\n",
    "    prob = input_exp / np.sum(input_exp, axis=1, keepdims=True)\n",
    "    return prob\n",
    "# 反向传播的代码，与后面的交叉熵损失函数结合实现"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T12:29:15.830209700Z",
     "start_time": "2024-06-16T12:29:15.806527100Z"
    }
   },
   "id": "2f7ac6d263b5365a",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 交叉熵损失函数公式\n",
    "## 4.1 交叉熵损失函数的前向计算\n",
    "$$\n",
    "L=-\\frac{1}{p}\\sum_i{\\boldsymbol{y}_i\\ln \\boldsymbol{\\hat{y}}_i} \\tag{4.1}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$p$是batch_size\n",
    "$\\boldsymbol{y}_i$是一个$num\\_class$维的one-hot向量, 其中样本$i$的真实标签对应的位置为1，其他位置为0\n",
    "$\\boldsymbol{\\hat{y}}_i$是softmax层的输出，表示样本$i$属于各个类别的概率\n",
    "$\\boldsymbol{y}_i\\ln \\boldsymbol{\\hat{y}}_i$相当于只保留了样本$i$属于真实标签类别的概率"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c773f95f88b97ebe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "考虑到${{\\hat{y}}_i}$趋近于0时，$\\ln{{\\hat{y}}_i}$可能会发生溢出，在代码实现时一般会结合防溢出softmax函数（公式(3.2)）一起使用：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log{(\\hat y_j)} & = \\log\\left( \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}\\right) \\\\\n",
    "& = \\log{(\\exp(o_j - \\max(o_k)))}-\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)} \\\\\n",
    "& = o_j - \\max(o_k) -\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)}\n",
    "\\end{aligned} \\tag{4.2}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$\\hat{y}$是一个$num\\_class$维的向量，$\\hat{y}_j$表示样本属于类别$j$的概率\n",
    "$\\log$实际上是以$e$为底的对数函数，即$\\ln$\n",
    "$o_j$是softmax层的输入，即未经过softmax函数的model的输出\n",
    "$\\max(o_k)$是维数为$k$的样本特征向量$o$中的最大项"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6ed0da40ad0d92d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 交叉熵损失函数的反向传播\n",
    "### 4.2.1 对输入$X$的梯度\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \t\n",
    "\\left( \\nabla _{\\boldsymbol{X}}L \\right) _{i,j}&=\\frac{\\partial L}{\\partial x_{i,j}}=\\frac{1}{p}\\left( \\boldsymbol{\\hat{y}}_{i,j}-\\boldsymbol{y}_{i,j} \\right)\\\\\n",
    "\\end{aligned}\\tag{4.3}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92bfa16c456cd17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "详细推导过程如下：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j}}=-\\frac{1}{p}\\sum_k{\\frac{\\boldsymbol{y}_{i,k}}{\\boldsymbol{\\hat{y}}_{i,k}}\\frac{\\partial \\boldsymbol{\\hat{y}}_{i,k}}{\\partial x_{i,j}}}\n",
    "$$\n",
    "根据softmax的求导公式(3.3)，拆分$\\frac{\\partial \\boldsymbol{\\hat{y}}_{i,k}}{\\partial x_{i,j}}$为两种情况：\n",
    "$$\n",
    "=-\\frac{1}{p}(\\sum_{k\\neq j}{\\frac{\\boldsymbol{y}_{i,k}}{\\boldsymbol{\\hat{y}}_{i,k}} (-\\boldsymbol{\\hat{y}}_{i,j}\\boldsymbol{\\hat{y}}_{i,k})+{\\frac{\\boldsymbol{y}_{i,j}}{\\boldsymbol{\\hat{y}}_{i,j}}(-\\boldsymbol{\\hat{y}}_{i,j}\\boldsymbol{\\hat{y}}_{i,j}+\\boldsymbol{\\hat{y}}_{i,j}))\n",
    "$$\n",
    "补齐缺项求和\n",
    "$$\n",
    "=\\frac{1}{p}(\\sum_k{\\frac{\\boldsymbol{y}_{i,k}}{\\boldsymbol{\\hat{y}}_{i,k}}\\boldsymbol{\\hat{y}}_{i,j}\\boldsymbol{\\hat{y}}_{i,k}}-{\\frac{\\boldsymbol{y}_{i,j}}{\\boldsymbol{\\hat{y}}_{i,j}}\\boldsymbol{\\hat{y}}_{i,j})\n",
    "$$\n",
    "化简分母\n",
    "$$\n",
    "=\\frac{1}{p}((\\sum_k{{\\boldsymbol{y}_{i,k}})\\boldsymbol{\\hat{y}}_{i,j}-\\boldsymbol{y}_{i,j})\n",
    "$$\n",
    "$\\sum_k{{\\boldsymbol{y}_{i,k}}=1$, 所以\n",
    "$$\n",
    "=\\frac{1}{p}\\left( \\boldsymbol{\\hat{y}}_{i,j}-\\boldsymbol{y}_{i,j} \\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bbb8588e8421b69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 交叉熵损失函数的代码实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a33c2ac570e114fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer:\n",
    "    def __init__(self):\n",
    "        self.prob = None\n",
    "        self.label_onehot = None\n",
    "\n",
    "    def forward(self, output, label):\n",
    "        # 备份概率值，反向传播时会用到\n",
    "        self.prob = softmax(output)\n",
    "        # 将标签转换为one-hot编码并备份\n",
    "        batch_size = self.prob.shape[0]\n",
    "        self.label_onehot = np.zeros_like(self.prob)\n",
    "        self.label_onehot[np.arange(batch_size), label] = 1.0\n",
    "        # 计算交叉熵损失，对应公式(4.1)\n",
    "        # loss = -np.sum(np.log(self.prob) * self.label_onehot) / batch_size\n",
    "        # 为了防止溢出，使用下面的计算方式，对应公式(4.2)\n",
    "        output_max = np.max(output, axis=1, keepdims=True)\n",
    "        log_prob = output - output_max - np.log(np.sum(np.exp(output - output_max), axis=1, keepdims=True))\n",
    "        loss = -np.sum(log_prob * self.label_onehot) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        # 反向传播计算输入的梯度，对应公式(4.3)\n",
    "        # 虽然我们在前向计算时使用了优化过的公式，但这些修改不影响梯度的计算\n",
    "        batch_size = self.prob.shape[0]\n",
    "        d_input = (self.prob - self.label_onehot) / batch_size\n",
    "        return d_input"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T12:29:15.858581500Z",
     "start_time": "2024-06-16T12:29:15.832515400Z"
    }
   },
   "id": "6f8ff40aa0252a8d",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 参考资料：\n",
    "https://zhuanlan.zhihu.com/p/380036598  \n",
    "https://zh.d2l.ai/chapter_multilayer-perceptrons/backprop.html  \n",
    "https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html  \n",
    "https://www.cnblogs.com/gczr/p/16345902.html  \n",
    "https://blog.csdn.net/chaipp0607/article/details/101946040  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8803b244d8eaaaa8"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c18a0ca4e69813ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
